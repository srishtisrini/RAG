# -*- coding: utf-8 -*-
"""Rag_AA5750.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DYKcmljKtkxyL0SFu6TlkV4yAJJXEOXc
"""

import os

base_dir ="/content/materials" # name of the folder materials
os.makedirs( base_dir, exist_ok= True) # makes folder if it already exits do not do anything
print("Folder created at:",base_dir)
print("Current files:",os.listdir(base_dir))#os.listdir give list of the folder indie base-dir that is material

!pip install pypdf

#Ingestion part
import os
from pypdf import PdfReader

base_dir = "/content/materials"

documents =[]
for filename in os.listdir(base_dir):
  if filename.lower().endswith(".pdf"):
    print("Processing:",filename)

    file_path = os.path.join(base_dir,filename)

    reader = PdfReader(file_path)
    full_text =""

    for page in reader.pages:
      page_text = page.extract_text()
      if page_text:
        full_text +=" " +page_text

    clean_text = " ".join(full_text.split())
    print("Length of extracted text",len(clean_text))

    doc ={
          "id":len(documents),
          "filename":filename,
          "text":clean_text
         }
    documents.append(doc)

print("\nTotal document loaded:",len(documents))

print("Total documents loaded:", len(documents))
print("First document keys:", documents[0].keys())
print("First filename:", documents[0]["filename"])
print("First 500 characters of text:\n")
print(documents[0]["text"][:500])

#chunking by words with chunk size
def chunk_text_by_words(text, chunk_size=200):
  words = text.split()
  chunks=[]

  for start in range(0,len(words),chunk_size):
    end = start + chunk_size
    chunk_words = words[start:end]
    chunk_text = " ".join(chunk_words)
    chunks.append(chunk_text)

  return chunks

chunks_fixed = []
for doc in documents:
  doc_chunks = chunk_text_by_words(doc["text"],chunk_size=200)
  for i,chunk in enumerate(doc_chunks):
    chunks_fixed.append({
        "doc_id": doc["id"],
        "chunk_index": i,
        "filename": doc["filename"],
        "text":chunk
    })
print("Total fixed-size chunks:", len(chunks_fixed))
print("Example chunk keys:", chunks_fixed[0].keys())
print("From file:", chunks_fixed[0]["filename"])
print("Chunk text (first 300 chars):")
print(chunks_fixed[0]["text"][:300])

#Chunking by sentence based
import re

def chunk_text_by_sentences(text, sentences_per_chunk=3):
    sentences = re.split(r'(?<=[.!?])\s+', text)
    chunks = []
    current_chunk = []

    for sent in sentences:
        sent = sent.strip()
        if not sent:
            continue

        current_chunk.append(sent)

        if len(current_chunk) >= sentences_per_chunk:
            chunk_text = " ".join(current_chunk)
            chunks.append(chunk_text)
            current_chunk = []

    if current_chunk:
        chunk_text = " ".join(current_chunk)
        chunks.append(chunk_text)

    return chunks

chunks_sentence = []

for doc in documents:
    doc_chunks = chunk_text_by_sentences(doc["text"], sentences_per_chunk=3)
    for i, chunk in enumerate(doc_chunks):
        chunks_sentence.append({
            "doc_id": doc["id"],
            "chunk_index": i,
            "filename": doc["filename"],
            "text": chunk
        })

print("Total sentence-based chunks:", len(chunks_sentence))
print("Example sentence-chunk keys:", chunks_sentence[0].keys())
print("From file:", chunks_sentence[0]["filename"])
print("Chunk text (first 300 chars):")
print(chunks_sentence[0]["text"][:300])

!pip install sentence-transformers

from sentence_transformers import SentenceTransformer
import numpy as np

model_open = SentenceTransformer("all-MiniLM-L6-v2")

texts_fixed = [chunk["text"] for chunk in chunks_fixed]

embeddings_open_fixed = model_open.encode(texts_fixed, batch_size = 32, show_progress_bar = True)

embeddings_open_fixed = np.array(embeddings_open_fixed)

print("Embeddings shape:", embeddings_open_fixed.shape)

def cosine_similarity(a,b):
   a = a / (np.linalg.norm(a)+ 1e-8)
   b = b / (np.linalg.norm(b, axis =1, keepdims =True) + 1e-8)
   sims = np.dot(b, a)

   return sims

def retrieve_top_k(query_text, model, chunk_texts, chunk_meta, embeddings_matrix, k=5):
    # 1. Embed the query using the same model
    query_emb = model.encode([query_text])[0]

    # 2. Compute cosine similarity between query and all chunk embeddings
    sims = cosine_similarity(query_emb, embeddings_matrix)

    # 3. Get indices of top-k highest similarity scores
    top_k_idx = np.argsort(-sims)[:k]

    # 4. Gather the results
    results = []
    for idx in top_k_idx:
        results.append({
            "score": float(sims[idx]),
            "text": chunk_texts[idx],
            "meta": chunk_meta[idx]
        })

    return results

meta_fixed = [
    {
        "doc_id": c["doc_id"],
        "chunk_index": c["chunk_index"],
        "filename": c["filename"]
    }
    for c in chunks_fixed
]

len(meta_fixed), len(texts_fixed), embeddings_open_fixed.shape[0]

query = "Explain what a dropout layer does in neural networks."

results = retrieve_top_k(
    query_text=query,
    model=model_open,
    chunk_texts=texts_fixed,
    chunk_meta=meta_fixed,
    embeddings_matrix=embeddings_open_fixed,
    k=5
)

print("Query:", query)
print("\nTop-5 retrieved chunks:\n")
for r in results:
    print("Score:", round(r["score"], 3))
    print("From file:", r["meta"]["filename"], "| chunk:", r["meta"]["chunk_index"])
    print(r["text"][:400], "...")
    print("-" * 80)

!pip install --upgrade openai

from openai import OpenAI
import numpy as np  # make sure NumPy is imported

# IMPORTANT: put your real API key instead of "YOUR_API_KEY_HERE" when you have one.
client = OpenAI(api_key="removed for safety concerns")

def get_openai_embeddings(text_list, model="text-embedding-3-small", batch_size=100):
    all_embeddings = []

    for start in range(0, len(text_list), batch_size):
        end = start + batch_size
        batch = text_list[start:end]

        response = client.embeddings.create(
            model=model,
            input=batch
        )

        for item in response.data:
            all_embeddings.append(item.embedding)

    return np.array(all_embeddings)

embeddings_openai_fixed = get_openai_embeddings(
    texts_fixed,
    model="text-embedding-3-small",
    batch_size=100
)

print("OpenAI embeddings shape:", embeddings_openai_fixed.shape)

# Build a list of sentence-based chunk texts
texts_sentence = [c["text"] for c in chunks_sentence]

# Compute open-source embeddings for sentence-based chunks
embeddings_open_sentence = model_open.encode(
    texts_sentence,
    batch_size=32,
    show_progress_bar=True
)
embeddings_open_sentence = np.array(embeddings_open_sentence)

# Build parallel metadata list for sentence chunks
meta_sentence = [
    {
        "doc_id": c["doc_id"],
        "chunk_index": c["chunk_index"],
        "filename": c["filename"]
    }
    for c in chunks_sentence
]

print("Sentence-based embeddings shape:", embeddings_open_sentence.shape)
print("Number of meta_sentence entries:", len(meta_sentence))
print("Number of texts_sentence entries:", len(texts_sentence))

def build_context_from_results(results, max_chars_per_chunk=600):
    """
    Take the list of retrieved chunks (results from retrieve_top_k)
    and build a single context string for the LLM.
    """
    parts = []
    for i, r in enumerate(results, start=1):
        text = r["text"]
        # optionally truncate each chunk so context doesn't get too long
        snippet = text[:max_chars_per_chunk]
        part = (
            f"[Chunk {i} | file: {r['meta']['filename']} | chunk_index: {r['meta']['chunk_index']}]\n"
            f"{snippet}\n"
        )
        parts.append(part)
    context = "\n".join(parts)
    return context

def generate_answer_with_rag_openai(query, results, model_name="gpt-4o-mini"):
    """
    query: user question (string)
    results: list from retrieve_top_k (retrieved chunks)
    model_name: OpenAI model for generation (e.g., gpt-4o-mini)
    """
    context = build_context_from_results(results)

    prompt = f"""
You are a teaching assistant for the course AA-5750: Contemporary Issues in Analytics.

Use ONLY the information in the CONTEXT below to answer the student's question.
If the answer is not covered in the context, say exactly: "I cannot answer this from the course materials."

QUESTION:
{query}

CONTEXT:
{context}

ANSWER (use clear, simple language, for a non-technical student):
"""

    response = client.responses.create(
        model=model_name,
        input=prompt
    )

    # The Python SDK exposes the main answer as .output_text
    return response.output_text

def generate_answer_without_rag_openai(query, model_name="gpt-4o-mini"):
    """
    Baseline: ask the LLM the question WITHOUT giving it any course context.
    """
    prompt = f"""
You are a general AI assistant (not limited to course materials).

Answer the following question as best as you can:

QUESTION:
{query}

ANSWER:
"""

    response = client.responses.create(
        model=model_name,
        input=prompt
    )

    return response.output_text

query = "Where is the dropout part?"

# 1) Retrieve top-k chunks using OPEN-SOURCE embeddings + FIXED chunks
results_fixed_open = retrieve_top_k(
    query_text=query,
    model=model_open,
    chunk_texts=texts_fixed,
    chunk_meta=meta_fixed,
    embeddings_matrix=embeddings_open_fixed,
    k=5
)

print("Top-5 retrieved chunks (open-source, fixed-size):\n")
for r in results_fixed_open:
    print("Score:", round(r["score"], 3), "| File:", r["meta"]["filename"], "| Chunk:", r["meta"]["chunk_index"])
print("\n" + "="*80 + "\n")

# 2) Generate answer WITH RAG (using retrieved context)
# NOTE: requires a valid OpenAI API key
# comment these lines out if you don't have a key yet

answer_with_rag = generate_answer_with_rag_openai(query, results_fixed_open)
print("ANSWER WITH RAG:\n")
print(answer_with_rag)

# 3) Generate answer WITHOUT RAG (no course context)
answer_without_rag = generate_answer_without_rag_openai(query)
print("\n" + "="*80 + "\n")
print("ANSWER WITHOUT RAG:\n")
print(answer_without_rag)